{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79567e1d",
   "metadata": {},
   "source": [
    "# Base de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10204aca",
   "metadata": {},
   "source": [
    "## POR TEMPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21113bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "# --- CONFIGURAÇÕES ---\n",
    "DATASET_PATH = 'datasets/CICDDoS2019/'\n",
    "\n",
    "ATTACK_ORDER = {\n",
    "    '03-11': [\n",
    "        'Portmap.csv', 'NetBIOS.csv', 'LDAP.csv', 'MSSQL.csv', 'UDP.csv', 'UDPLag.csv', 'Syn.csv'\n",
    "    ],\n",
    "    '01-12': [\n",
    "        'DrDoS_NTP.csv', 'DrDoS_DNS.csv', 'DrDoS_LDAP.csv', 'DrDoS_MSSQL.csv', 'DrDoS_NetBIOS.csv', 'DrDoS_SNMP.csv', 'DrDoS_SSDP.csv', 'DrDoS_UDP.csv', \n",
    "        'UDPLag.csv', 'Syn.csv', 'TFTP.csv' \n",
    "    ]\n",
    "}\n",
    "\n",
    "OUTPUT_FILES = {\n",
    "    '03-11': 'CICDDoS2019_03_11_Aggregated_Features_1sWindow.csv', \n",
    "    '01-12': 'CICDDoS2019_01_12_Aggregated_Features_1sWindow.csv' \n",
    "}\n",
    "\n",
    "# Tamanho do chunking \n",
    "PANDAS_CHUNK_SIZE = 100000 \n",
    "\n",
    "# Tamanho da janela temporal \n",
    "TIME_WINDOW_SECONDS = 1.0\n",
    "\n",
    "# Colunas que serão tratadas de forma especial\n",
    "TIMESTAMP_COL = 'Timestamp'\n",
    "ATTACK_LABEL_COL = 'Label' \n",
    "\n",
    "# Colunas para cálculo de Entropia \n",
    "DIVERSITY_COLS = ['Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Protocol'] \n",
    "\n",
    "# Colunas lixo a ser removidas \n",
    "COLUMNS_TO_DROP = ['Unnamed: 0', 'Flow ID', 'SimillarHTTP']\n",
    "\n",
    "# Constante para o rótulo Benigno\n",
    "BENIGN_LABEL = 'BENIGN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0229bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(columns: pd.Index) -> List[str]:\n",
    "    return [col.replace(' ', '_') for col in columns]\n",
    "\n",
    "def shannon_entropy(data: pd.Series) -> float:\n",
    "    if data.empty:\n",
    "        return 0.0\n",
    "    \n",
    "    data = data.astype(str).dropna()\n",
    "    counts = data.value_counts(normalize=True)\n",
    "    entropy = -np.sum(counts * np.log2(counts))\n",
    "    return entropy\n",
    "\n",
    "def aggregate_window_by_time(df_window: pd.DataFrame) -> pd.Series:\n",
    "    timestamp_col_clean = TIMESTAMP_COL.replace(' ', '_')\n",
    "    attack_label_col_clean = ATTACK_LABEL_COL.replace(' ', '_')\n",
    "    diversity_cols_clean = [col.replace(' ', '_') for col in DIVERSITY_COLS]\n",
    "    \n",
    "    # Identifica colunas numéricas \n",
    "    numeric_cols = [col for col in df_window.columns \n",
    "                    if col not in [timestamp_col_clean, attack_label_col_clean] + diversity_cols_clean]\n",
    "\n",
    "    # Calcular a Média para Colunas Numéricas\n",
    "    numeric_data = df_window[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    aggregated_row = numeric_data.mean(axis=0)\n",
    "\n",
    "    # Agregação Temporal\n",
    "    try:\n",
    "        # Garante que o Timestamp seja tratado como datetime, erros se tornam NaT\n",
    "        timestamps = pd.to_datetime(df_window[timestamp_col_clean], errors='coerce')\n",
    "        valid_timestamps = timestamps.dropna()\n",
    "        \n",
    "        delta_seconds = 0.0\n",
    "        start_time = None\n",
    "        \n",
    "        if len(valid_timestamps) >= 2:\n",
    "            delta_seconds = (valid_timestamps.iloc[-1] - valid_timestamps.iloc[0]).total_seconds()\n",
    "        \n",
    "        if not valid_timestamps.empty:\n",
    "            start_time = valid_timestamps.iloc[0].strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        \n",
    "    except Exception:\n",
    "        delta_seconds = 0.0 \n",
    "        start_time = 'Error'\n",
    "\n",
    "    aggregated_row[f'{timestamp_col_clean}_Delta_Seconds'] = delta_seconds\n",
    "    aggregated_row[f'{timestamp_col_clean}_Start'] = start_time\n",
    "    \n",
    "    if timestamp_col_clean in aggregated_row.index:\n",
    "         aggregated_row = aggregated_row.drop(timestamp_col_clean)\n",
    "\n",
    "    # Entropia de Shannon \n",
    "    for col_clean in diversity_cols_clean:\n",
    "        entropy = shannon_entropy(df_window[col_clean].astype(str).dropna())\n",
    "        aggregated_row[f'{col_clean}_Shannon_Entropy'] = entropy\n",
    "\n",
    "\n",
    "      # O rótulo final é o mais frequente no intervalo de 1 segundo\n",
    "    attack_labels = df_window[attack_label_col_clean].astype(str).str.strip().str.upper().replace('NAN', BENIGN_LABEL).dropna()\n",
    "    final_string_label = attack_labels.mode().iloc[0] if not attack_labels.empty else BENIGN_LABEL\n",
    "\n",
    "    # MANTÉM O RÓTULO AGREGADO NO LUGAR DA COLUNA ORIGINAL\n",
    "    aggregated_row[attack_label_col_clean] = final_string_label \n",
    "    aggregated_row['Window_Packet_Count'] = len(df_window)\n",
    "    \n",
    "    return aggregated_row.to_frame().T\n",
    "\n",
    "\n",
    "def concatenate_and_aggregate(date_folder: str, output_filename: str, data_path: str, file_names: List[str]):\n",
    "    ordered_files = []\n",
    "    base_path = os.path.join(data_path, date_folder)\n",
    "    \n",
    "    if not os.path.isdir(base_path):\n",
    "        print(f\"O caminho base '{base_path}' não foi encontrado.\")\n",
    "        return\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            ordered_files.append(file_path)\n",
    "    if not ordered_files:\n",
    "        print(f\"Nenhum arquivo encontrado para o padrão '{base_path}/*.csv'.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nProcessando data: {date_folder} ({len(ordered_files)} arquivos)\")\n",
    "    print(f\"-> Escrevendo o arquivo agregado para: {output_filename}\")\n",
    "    \n",
    "    global_first_write = True\n",
    "    \n",
    "    # Buffer unificado para janelamento temporal\n",
    "    aggregation_buffer = pd.DataFrame() \n",
    "\n",
    "    for file_path in ordered_files:\n",
    "        print(f\"-> Concatenando e agregando arquivo: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        try:\n",
    "            chunker = pd.read_csv(\n",
    "                file_path, \n",
    "                chunksize=PANDAS_CHUNK_SIZE, \n",
    "                low_memory=False, \n",
    "                skipinitialspace=True,\n",
    "                dtype=str \n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERRO ao abrir o arquivo {os.path.basename(file_path)}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for chunk in chunker:\n",
    "            \n",
    "            chunk.columns = clean_column_names(chunk.columns)\n",
    "            \n",
    "            # Remove as colunas\n",
    "            cols_to_drop_clean = [col.replace(' ', '_') for col in COLUMNS_TO_DROP]\n",
    "            chunk = chunk.drop(columns=cols_to_drop_clean, errors='ignore')\n",
    "            \n",
    "            # Concatena o chunk lido ao buffer\n",
    "            aggregation_buffer = pd.concat([aggregation_buffer, chunk], ignore_index=True)\n",
    "            \n",
    "            # Converte timestamps no buffer \n",
    "            buffer_ts = pd.to_datetime(aggregation_buffer[TIMESTAMP_COL.replace(' ', '_')], errors='coerce')\n",
    "            valid_ts_buffer = buffer_ts.dropna()\n",
    "            \n",
    "            if valid_ts_buffer.empty:\n",
    "                continue\n",
    "\n",
    "            # Início da janela de tempo é o primeiro timestamp válido no buffer\n",
    "            start_time = valid_ts_buffer.iloc[0]\n",
    "            \n",
    "            # Encontra o índice da última linha que está DENTRO da janela de 1 segundo\n",
    "            time_diffs = (valid_ts_buffer - start_time).dt.total_seconds()\n",
    "            \n",
    "            # Índices de todas as linhas que estão dentro da janela de 1s\n",
    "            window_indices = time_diffs[time_diffs < TIME_WINDOW_SECONDS].index\n",
    "            \n",
    "            # Verifica se há linhas suficientes para fechar uma janela de tempo\n",
    "            while not window_indices.empty:\n",
    "                # O último índice válido que faz parte da janela de 1s\n",
    "                last_index_in_window = window_indices[-1]\n",
    "\n",
    "                # A janela é tudo do início até o último índice\n",
    "                window = aggregation_buffer.iloc[:last_index_in_window + 1]\n",
    "                \n",
    "                # Agrega a janela\n",
    "                aggregated_row_df = aggregate_window_by_time(window)\n",
    "                \n",
    "                # Escrita\n",
    "                header = global_first_write\n",
    "                mode = 'w' if global_first_write else 'a'\n",
    "                aggregated_row_df.to_csv(output_filename, mode=mode, header=header, index=False)\n",
    "                global_first_write = False\n",
    "                \n",
    "                # Remove a janela processada do buffer e reseta o index\n",
    "                aggregation_buffer = aggregation_buffer.iloc[last_index_in_window + 1:].reset_index(drop=True)\n",
    "\n",
    "                # Re-calcula os tempos e índices para o próximo loop\n",
    "                buffer_ts = pd.to_datetime(aggregation_buffer[TIMESTAMP_COL.replace(' ', '_')], errors='coerce')\n",
    "                valid_ts_buffer = buffer_ts.dropna()\n",
    "                \n",
    "                if valid_ts_buffer.empty:\n",
    "                    window_indices = pd.Index([]) # Força saída do while\n",
    "                else:\n",
    "                    start_time = valid_ts_buffer.iloc[0]\n",
    "                    time_diffs = (valid_ts_buffer - start_time).dt.total_seconds()\n",
    "                    window_indices = time_diffs[time_diffs < TIME_WINDOW_SECONDS].index\n",
    "    \n",
    "    # Processa o que sobrou no buffer como a última janela \n",
    "    if not aggregation_buffer.empty and global_first_write:\n",
    "        # Se o buffer não estiver vazio e não houver sido escrito nada\n",
    "        aggregated_row_df = aggregate_window_by_time(aggregation_buffer)\n",
    "        aggregated_row_df.to_csv(output_filename, mode='w', header=True, index=False)\n",
    "    elif not aggregation_buffer.empty:\n",
    "        # Se o buffer não estiver vazio e já houver sido escrito algo\n",
    "        aggregated_row_df = aggregate_window_by_time(aggregation_buffer)\n",
    "        aggregated_row_df.to_csv(output_filename, mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "    print(f\"--- Processamento concluído para {date_folder}. O arquivo '{output_filename}' foi criado. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9aa441",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for date, output_file in OUTPUT_FILES.items():\n",
    "        concatenate_and_aggregate(date, output_file, DATASET_PATH, ATTACK_ORDER[date])\n",
    "\n",
    "    print(\"\\nProcesso de agregação e engenharia de features concluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942251e",
   "metadata": {},
   "source": [
    "# Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8428609d",
   "metadata": {},
   "source": [
    "## Active Learning (AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f880f",
   "metadata": {},
   "source": [
    "### Expected Model Change (EMC)\n",
    "\n",
    "O EMC seleciona a amostra cuja rotulagem causaria a maior mudança nos pesos (gradientes) do modelo. Se o modelo tem que se esforçar muito para incorporar um novo dado, esse dado é valioso. Isso garante que cada rótulo pago (custo) maximize o aprendizado do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58965589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b120ab6",
   "metadata": {},
   "source": [
    "### Budgeted Active Learning (BAL)\n",
    "\n",
    "O BAL não é um algoritmo de seleção, mas um quadro de gerenciamento. Seu objetivo é garantir que o Active Learning opere de forma sustentável, respeitando o limite (orçamento ou budget) de tempo, dinheiro ou amostras que podem ser rotuladas em um dado período de tempo (crucial em Data Streams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84debf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7885d0e",
   "metadata": {},
   "source": [
    "### Density-Weighted Methods\n",
    "\n",
    "Combina a Incerteza (o modelo está em dúvida) com a Densidade da amostra no espaço de features. Isso evita rotular anomalias únicas (outliers), que são incertas, mas de pouco valor para a fronteira de decisão principal. Ele busca amostras incertas que também são representativas de uma grande parte do Data Stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da724692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7efac71c",
   "metadata": {},
   "source": [
    "## Semisupervisionado (SSL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b76bf",
   "metadata": {},
   "source": [
    "### Semi-supervised Multi-view Stream Cluster (SmSCluster)\n",
    "\n",
    "Estratégia especializada que usa técnicas de agrupamento (clustering) para dados de fluxo com poucos rótulos. Ele aproveita a similaridade dos dados (clusters) para propagar rótulos (ou pseudo-rótulos) dentro de grupos coesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09bc635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f3c0c56",
   "metadata": {},
   "source": [
    "### Semi-Supervised Ensemble Algorithm\n",
    "\n",
    "Estrutura genérica que combina múltiplos classificadores (ensemble) e os treina usando uma mistura de dados rotulados e não rotulados. O foco é usar o consenso do ensemble para gerar pseudo-rótulos mais robustos (evitando erros de um único modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ccfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a819cbf8",
   "metadata": {},
   "source": [
    "### Online Semi-supervised Neural Network (OSNN)\n",
    "\n",
    "Um algoritmo que integra a capacidade de aprendizado incremental (Online Learning) com otimização semissupervisionada (geralmente usando o princípio de consistência ou propagação de rótulos) dentro de uma estrutura de Rede Neural (ex: RBF Network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33817e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf3c497e",
   "metadata": {},
   "source": [
    "## Concept Drift (CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac6fc2",
   "metadata": {},
   "source": [
    "### Adaptive Windowing (ADWIN)\n",
    "\n",
    "Detector de mudança estatística que monitora o erro de classificação (ou a média das features) em uma janela de tamanho variável, adaptando-se rapidamente à mudança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f9c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e30230",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov Windowing (KSWIN)\n",
    "\n",
    "Detector que utiliza o teste estatístico de Kolmogorov-Smirnov para verificar se os dados de duas sub-janelas têm distribuições de probabilidade diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f36abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21662250",
   "metadata": {},
   "source": [
    "### Drift Detection Method (DDM) e Early DDM (EDDM)\n",
    "\n",
    "DDM monitora o erro do classificador usando limites baseados na Desigualdade de Hoeffding; EDDM é uma variação otimizada para detectar drifts graduais ou lentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef902890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61515204",
   "metadata": {},
   "source": [
    "### Online Density Separation (ODS)\n",
    "\n",
    "Monitora a mudança na separação de densidade entre as classes. Tipicamente, utiliza um método não supervisionado (como Isolation Forest ou Autoencoders) para pontuar o quão anômalo é o tráfego atual. O drift é sinalizado quando a média dessas pontuações de anomalia muda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0605b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7119e4",
   "metadata": {},
   "source": [
    "## Online Machine Learning (OML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c0529",
   "metadata": {},
   "source": [
    "### Hoeffding Trees (HT) \n",
    "\n",
    "É um classificador de árvore de decisão incremental. Ele usa a Desigualdade de Hoeffding para decidir, com base em evidências estatísticas, o melhor momento para dividir um nó, garantindo que a árvore construída online seja similar àquela construída com todos os dados offline. É a base para muitos outros métodos de OML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea6b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eb53ec2",
   "metadata": {},
   "source": [
    "### Adaptive Random Forests (ARF)\n",
    "\n",
    "Um dos ensembles mais eficientes para Data Streams. Combina o Online Bagging com um mecanismo de substituição de árvores degradadas. Ele utiliza detectores de Concept Drift (como o ADWIN) para substituir sub-árvores que se tornaram obsoletas, permitindo rápida adaptação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0091a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68b2523d",
   "metadata": {},
   "source": [
    "### OzaBagging\n",
    "\n",
    "Implementação do Online Bagging. Treina um ensemble (conjunto) de classificadores em diferentes subamostras (com reposição) do stream de dados. O OzaBagging e o Online Boosting mantêm a diversidade do ensemble e aumentam a acurácia em streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dea4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa93688",
   "metadata": {},
   "source": [
    "### Dynamic Weighted Majority (DWM)\n",
    "\n",
    "Estrutura que atribui pesos dinâmicos aos classificadores. O DWM ajusta os pesos dos modelos que erram e pode criar novos classificadores quando o ensemble atinge um erro alto (sinal de Concept Drift). É conhecido por sua capacidade de se adaptar a drifts abruptos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3dceb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6f91c61",
   "metadata": {},
   "source": [
    "### Online Bagging with Active Learning (OBA)\n",
    "\n",
    "O OBA é uma variação do Online Bagging que integra a função de consulta do Active Learning. Ele usa o ensemble para determinar as amostras de dados de stream que mais beneficiarão o aprendizado se forem rotuladas por um especialista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a420bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bfbe6dd",
   "metadata": {},
   "source": [
    "## Pipeline Inicial (HT+ADWIN+BAL+Pseudo Rotulagem por confiança)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf527f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
