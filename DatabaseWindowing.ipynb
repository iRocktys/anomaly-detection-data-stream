{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e70605",
   "metadata": {},
   "source": [
    "# Janelamento das Amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e713d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import os\n",
    "\n",
    "def calcular_entropia(series):\n",
    "    if series.empty:\n",
    "        return 0\n",
    "    counts = series.value_counts()\n",
    "    return entropy(counts, base=2)\n",
    "\n",
    "def processar_dataset_temporal(input_path, output_path, freq='1s', chunk_size=200000):\n",
    "    reader = pd.read_csv(\n",
    "        input_path, \n",
    "        chunksize=chunk_size, \n",
    "        low_memory=True, \n",
    "        engine='python', \n",
    "        on_bad_lines='warn', \n",
    "        sep=None             \n",
    "    )\n",
    "    \n",
    "    primeiro_chunk = True\n",
    "\n",
    "    for i, chunk in enumerate(reader):\n",
    "        # Limpeza básica de nomes de colunas\n",
    "        chunk.columns = chunk.columns.str.strip()\n",
    "        \n",
    "        # Converte Timestamp tratando erros\n",
    "        chunk['Timestamp'] = pd.to_datetime(chunk['Timestamp'], errors='coerce')\n",
    "        chunk = chunk.dropna(subset=['Timestamp']).set_index('Timestamp')\n",
    "        \n",
    "        dict_agg = {}\n",
    "        cols_entropia = []\n",
    "\n",
    "        for col in chunk.columns:\n",
    "            # Entropia para Identificadores\n",
    "            if any(x in col for x in ['IP', 'Port', 'Protocol']):\n",
    "                dict_agg[col] = calcular_entropia\n",
    "                cols_entropia.append(col)\n",
    "            \n",
    "            # Label (Moda ou BENIGN)\n",
    "            elif col == 'Label':\n",
    "                dict_agg[col] = lambda x: x.mode()[0] if not x.empty else 'BENIGN'\n",
    "            \n",
    "            # Somatório para colunas numéricas\n",
    "            elif pd.api.types.is_numeric_dtype(chunk[col]):\n",
    "                dict_agg[col] = 'sum'\n",
    "\n",
    "        # Executa o Janelamento \n",
    "        resumo = chunk.resample(freq).agg(dict_agg)\n",
    "        \n",
    "        # Preenche gaps temporais\n",
    "        if 'Label' in resumo.columns:\n",
    "            resumo['Label'] = resumo['Label'].fillna('BENIGN')\n",
    "            resumo['Label'] = resumo['Label'].replace('N/A', 'BENIGN')\n",
    "        \n",
    "        resumo = resumo.fillna(0)\n",
    "\n",
    "        # Renomeia colunas de Entropia\n",
    "        rename_map = {col: f\"{col}_Entropy\" if col in cols_entropia else col for col in resumo.columns}\n",
    "        resumo = resumo.rename(columns=rename_map)\n",
    "\n",
    "        # Garante que Label seja a última coluna\n",
    "        final_cols = [c for c in resumo.columns if c != 'Label']\n",
    "        if 'Label' in resumo.columns:\n",
    "            final_cols.append('Label')\n",
    "        \n",
    "        resumo = resumo[final_cols]\n",
    "\n",
    "        # Salva o progresso\n",
    "        header_bool = True if primeiro_chunk else False\n",
    "        mode_str = 'w' if primeiro_chunk else 'a'\n",
    "        \n",
    "        resumo.to_csv(output_path, mode=mode_str, header=header_bool)\n",
    "        primeiro_chunk = False\n",
    "\n",
    "    print(f\"Processamento concluído. Arquivo salvo: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d453903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento concluído. Arquivo salvo: DrDoS_SNMP_1s_0112.csv\n"
     ]
    }
   ],
   "source": [
    "ARQUIVO_ENTRADA = 'datasets/CICDDoS2019/01-12/DrDoS_SNMP.csv' \n",
    "ARQUIVO_SAIDA = 'DrDoS_SNMP_1s_0112.csv'\n",
    "FREQUENCIA = '1s'  # Ex: '1S' (1 seg), '500L' (500ms), '200L' (200ms)              \n",
    "    \n",
    "processar_dataset_temporal(\n",
    "    input_path=ARQUIVO_ENTRADA, \n",
    "    output_path=ARQUIVO_SAIDA, \n",
    "    freq=FREQUENCIA,\n",
    "    chunk_size=150000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7808086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento concluído. Arquivo salvo: DrDoS_DNS_1s_0112.csv\n"
     ]
    }
   ],
   "source": [
    "ARQUIVO_ENTRADA = 'datasets/CICDDoS2019/01-12/DrDoS_DNS.csv' \n",
    "ARQUIVO_SAIDA = 'DrDoS_DNS_1s_0112.csv'\n",
    "FREQUENCIA = '1s'  # Ex: '1S' (1 seg), '500L' (500ms), '200L' (200ms)            \n",
    "    \n",
    "processar_dataset_temporal(\n",
    "    input_path=ARQUIVO_ENTRADA, \n",
    "    output_path=ARQUIVO_SAIDA, \n",
    "    freq=FREQUENCIA,\n",
    "\n",
    "    chunk_size=150000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f39515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento concluído. Arquivo salvo: BENIGN_1s_0112.csv\n"
     ]
    }
   ],
   "source": [
    "ARQUIVO_ENTRADA = 'datasets/CICDDoS2019/data/benign_0112.csv' \n",
    "ARQUIVO_SAIDA = 'BENIGN_1s_0112.csv'\n",
    "FREQUENCIA = '1s'  # Ex: '1S' (1 seg), '500L' (500ms), '200L' (200ms)           \n",
    "    \n",
    "processar_dataset_temporal(\n",
    "    input_path=ARQUIVO_ENTRADA, \n",
    "    output_path=ARQUIVO_SAIDA, \n",
    "    freq=FREQUENCIA,\n",
    "    chunk_size=150000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d34643",
   "metadata": {},
   "source": [
    "# Mesclagem dos ataques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97d9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "def mesclar_ataques_no_benigno(\n",
    "    path_benign, \n",
    "    list_paths_attacks, \n",
    "    output_path, \n",
    "    chunk_size=200000\n",
    "):\n",
    "    df_attacks = pd.DataFrame()\n",
    "    \n",
    "    for atk_path in list_paths_attacks:\n",
    "        if os.path.exists(atk_path):\n",
    "            try:\n",
    "                df_temp = pd.read_csv(atk_path, low_memory=False, on_bad_lines='skip')\n",
    "                df_temp.columns = df_temp.columns.str.strip()\n",
    "                \n",
    "                # Garante datetime\n",
    "                df_temp['Timestamp'] = pd.to_datetime(df_temp['Timestamp'], errors='coerce')\n",
    "                df_temp = df_temp.dropna(subset=['Timestamp'])\n",
    "                \n",
    "                df_attacks = pd.concat([df_attacks, df_temp])\n",
    "            except Exception as e:\n",
    "                print(f\"   ERRO ao ler {atk_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"   AVISO: Arquivo não encontrado: {atk_path}\")\n",
    "\n",
    "    if df_attacks.empty:\n",
    "        print(\"ERRO CRÍTICO: Nenhum dado de ataque válido carregado. Abortando.\")\n",
    "        return\n",
    "\n",
    "    # Remove duplicatas de timestamps nos ataques\n",
    "    df_attacks = df_attacks.drop_duplicates(subset=['Timestamp'], keep='last')\n",
    "    \n",
    "    # Define Timestamp como índice para substituição rápida\n",
    "    df_attacks.set_index('Timestamp', inplace=True)\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "    \n",
    "    first_chunk = True\n",
    "    total_processed = 0\n",
    "    total_replaced = 0\n",
    "\n",
    "    # Adicionamos on_bad_lines aqui também por segurança\n",
    "    with pd.read_csv(path_benign, chunksize=chunk_size, low_memory=False, on_bad_lines='skip') as reader:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            \n",
    "            chunk['Timestamp'] = pd.to_datetime(chunk['Timestamp'], errors='coerce')\n",
    "            \n",
    "            # Guardamos a coluna Timestamp original para resetar depois\n",
    "            chunk.set_index('Timestamp', inplace=True)\n",
    "            \n",
    "            # Verifica interseção de índices \n",
    "            common_indices = chunk.index.intersection(df_attacks.index)\n",
    "            \n",
    "            if not common_indices.empty:\n",
    "                chunk.loc[common_indices] = df_attacks.loc[common_indices]\n",
    "                total_replaced += len(common_indices)\n",
    "            \n",
    "            chunk.reset_index(inplace=True)\n",
    "            \n",
    "            # Tratamento de N/A e preenchimento\n",
    "            chunk['Label'] = chunk['Label'].fillna('BENIGN')\n",
    "            chunk = chunk.fillna(0)\n",
    "\n",
    "            mode = 'w' if first_chunk else 'a'\n",
    "            header = first_chunk\n",
    "            chunk.to_csv(output_path, mode=mode, header=header, index=False)\n",
    "            \n",
    "            total_processed += len(chunk)\n",
    "            first_chunk = False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ef4c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARQUIVO_BENIGNO = \"BENIGN_1s_0112.csv\"\n",
    "ARQUIVO_FINAL = \"SNMP_DNS_1s_0112.csv\"\n",
    "LISTA_ATAQUES = [\n",
    "    \"DrDoS_SNMP_1s_0112.csv\",\n",
    "    \"DrDoS_DNS_1s_0112.csv\"\n",
    "]\n",
    "\n",
    "mesclar_ataques_no_benigno(ARQUIVO_BENIGNO, LISTA_ATAQUES, ARQUIVO_FINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa6c92",
   "metadata": {},
   "source": [
    "# Rotulagem e Filtro MURILO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f217a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def filter_and_label_by_time(\n",
    "    raw_input_path, \n",
    "    final_output_path, \n",
    "    global_start_str, \n",
    "    global_end_str, \n",
    "    attacks_ground_truth=None,\n",
    "    timestamp_col='timestamp', \n",
    "    date_format='%Y-%m-%d %H:%M:%S', \n",
    "    chunk_size=50000\n",
    "):\n",
    "    \n",
    "    print(f\"Período Global: {global_start_str} até {global_end_str}\")\n",
    "    print(f\"Formato de Data esperado: {date_format}\")\n",
    "    \n",
    "    # Converte limites globais usando o formato explícito\n",
    "    try:\n",
    "        global_start = pd.to_datetime(global_start_str, format=date_format)\n",
    "        global_end = pd.to_datetime(global_end_str, format=date_format)\n",
    "    except ValueError as e:\n",
    "        print(f\"ERRO: As datas de início/fim passadas não batem com o formato {date_format}.\")\n",
    "        print(f\"Detalhe: {e}\")\n",
    "        return\n",
    "\n",
    "    # Prepara lista de ataques\n",
    "    attack_rules = []\n",
    "    if attacks_ground_truth:\n",
    "        for start, end, label in attacks_ground_truth:\n",
    "            start_dt = pd.to_datetime(start, format=date_format)\n",
    "            end_dt = pd.to_datetime(end, format=date_format)\n",
    "            attack_rules.append((start_dt, end_dt, label))\n",
    "    \n",
    "    if os.path.exists(final_output_path):\n",
    "        os.remove(final_output_path)\n",
    "        print(f\"Arquivo antigo removido: {final_output_path}\")\n",
    "        \n",
    "    first_chunk = True\n",
    "    total_rows_saved = 0\n",
    "    with pd.read_csv(raw_input_path, chunksize=chunk_size, low_memory=False) as reader:\n",
    "        \n",
    "        for i, chunk in enumerate(reader):\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            \n",
    "            if timestamp_col not in chunk.columns:\n",
    "                print(f\"ERRO: Coluna '{timestamp_col}' não encontrada neste chunk.\")\n",
    "                print(f\"Colunas disponíveis: {chunk.columns.tolist()}\")\n",
    "                break \n",
    "            try:\n",
    "                chunk[timestamp_col] = pd.to_datetime(chunk[timestamp_col], format=date_format, errors='coerce')\n",
    "            except Exception as e:\n",
    "                print(f\"Aviso no Chunk {i}: Formato falhou, tentando inferência lenta...\")\n",
    "                chunk[timestamp_col] = pd.to_datetime(chunk[timestamp_col], errors='coerce')\n",
    "\n",
    "            # Remove linhas onde a data virou NaT (Not a Time) por erro de formatação\n",
    "            original_len = len(chunk)\n",
    "            chunk = chunk.dropna(subset=[timestamp_col])\n",
    "            if len(chunk) < original_len:\n",
    "                print(f\"   -> Aviso: {original_len - len(chunk)} linhas removidas por data inválida no chunk {i+1}\")\n",
    "            \n",
    "            # FILTRAGEM\n",
    "            mask_global = (chunk[timestamp_col] >= global_start) & (chunk[timestamp_col] <= global_end)\n",
    "            df_filtered = chunk.loc[mask_global].copy()\n",
    "            \n",
    "            if df_filtered.empty:\n",
    "                continue\n",
    "\n",
    "            # ROTULAGEM\n",
    "            df_filtered['Label'] = 'BENIGN'\n",
    "            \n",
    "            for atk_start, atk_end, atk_name in attack_rules:\n",
    "                mask_attack = (df_filtered[timestamp_col] >= atk_start) & \\\n",
    "                              (df_filtered[timestamp_col] <= atk_end)\n",
    "                df_filtered.loc[mask_attack, 'Label'] = atk_name\n",
    "            \n",
    "            # SALVAMENTO\n",
    "            mode = 'w' if first_chunk else 'a'\n",
    "            header = first_chunk\n",
    "            \n",
    "            df_filtered.to_csv(final_output_path, mode=mode, header=header, index=False)\n",
    "            \n",
    "            total_rows_saved += len(df_filtered)\n",
    "            first_chunk = False\n",
    "\n",
    "    print(f\"\\nConcluído! Arquivo salvo em: {final_output_path}\")\n",
    "    print(f\"Total de linhas no dataset final: {total_rows_saved}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e762562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Iniciando processamento...\n",
      "Período Global: 2025-10-13 12:00:00 até 2025-10-13 18:20:00\n",
      "Formato de Data esperado: %Y-%m-%d %H:%M:%S\n",
      "\n",
      ">>> Concluído! Arquivo salvo em: syn.csv\n",
      "Total de linhas no dataset final: 22776\n"
     ]
    }
   ],
   "source": [
    "MEU_FORMATO = '%Y-%m-%d %H:%M:%S' \n",
    "PERIODO_INICIO = \"2025-10-13 12:00:00\"\n",
    "PERIODO_FIM    = \"2025-10-13 18:20:00\"\n",
    "ATAQUES = [\n",
    "    (\"2025-10-13 15:26:30\", \"2025-10-13 17:16:20\", \"Syn\")\n",
    "]\n",
    "ARQUIVO_BRUTO = \"datasets/MURILO/synack/test_features_1s.txt\" \n",
    "ARQUIVO_FINAL = \"syn.csv\"\n",
    "\n",
    "\n",
    "# Chame a função passando o date_format\n",
    "filter_and_label_by_time(\n",
    "    raw_input_path=ARQUIVO_BRUTO,\n",
    "    final_output_path=ARQUIVO_FINAL,\n",
    "    global_start_str=PERIODO_INICIO,\n",
    "    global_end_str=PERIODO_FIM,\n",
    "    attacks_ground_truth=ATAQUES,\n",
    "    date_format=MEU_FORMATO,   \n",
    "    chunk_size=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ab0c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Período Global: 2025-07-16 17:00:00 até 2025-07-17 03:00:00\n",
      "Formato de Data esperado: %Y-%m-%d %H:%M:%S\n",
      "Arquivo antigo removido: datasets/MURILO/http-flood.csv\n",
      "\n",
      "Concluído! Arquivo salvo em: datasets/MURILO/http-flood.csv\n",
      "Total de linhas no dataset final: 35715\n"
     ]
    }
   ],
   "source": [
    "MEU_FORMATO = '%Y-%m-%d %H:%M:%S' \n",
    "PERIODO_INICIO = \"2025-07-16 17:00:00\"\n",
    "PERIODO_FIM    = \"2025-07-17 03:00:00\"\n",
    "ATAQUES = [\n",
    "    (\"2025-07-16 20:27:14\", \"2025-07-16 20:32:14\", \"http-flood\"),\n",
    "    (\"2025-07-16 22:23:07\", \"2025-07-16 22:23:07\", \"http-flood\"),\n",
    "    (\"2025-07-16 22:24:57\", \"2025-07-16 22:25:03\", \"http-flood\"),\n",
    "    (\"2025-07-16 22:33:05\", \"2025-07-16 22:33:07\", \"http-flood\"),\n",
    "    (\"2025-07-17 00:09:16\", \"2025-07-17 00:09:19\", \"http-flood\"),\n",
    "    (\"2025-07-17 00:30:15\", \"2025-07-17 00:30:17\", \"http-flood\")\n",
    "]\n",
    "ARQUIVO_BRUTO = \"datasets/MURILO/http-flood/test_features_1s.txt\" \n",
    "ARQUIVO_FINAL = \"datasets/MURILO/http-flood.csv\"\n",
    "\n",
    "\n",
    "# Chame a função passando o date_format\n",
    "filter_and_label_by_time(\n",
    "    raw_input_path=ARQUIVO_BRUTO,\n",
    "    final_output_path=ARQUIVO_FINAL,\n",
    "    global_start_str=PERIODO_INICIO,\n",
    "    global_end_str=PERIODO_FIM,\n",
    "    attacks_ground_truth=ATAQUES,\n",
    "    date_format=MEU_FORMATO,   \n",
    "    chunk_size=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded20188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Período Global: 2025-10-15 17:00:00 até 2025-10-16 02:04:59\n",
      "Formato de Data esperado: %Y-%m-%d %H:%M:%S\n",
      "\n",
      "Concluído! Arquivo salvo em: datasets/MURILO/udp.csv\n",
      "Total de linhas no dataset final: 32700\n"
     ]
    }
   ],
   "source": [
    "MEU_FORMATO = '%Y-%m-%d %H:%M:%S' \n",
    "PERIODO_INICIO = \"2025-10-15 17:00:00\"\n",
    "PERIODO_FIM    = \"2025-10-16 02:04:59\"\n",
    "ATAQUES = [\n",
    "    (\"2025-10-15 20:28:40\", \"2025-10-15 20:42:10\", \"udp\"),\n",
    "    (\"2025-10-15 20:53:40\", \"2025-10-15 21:06:20\", \"udp\"),\n",
    "    (\"2025-10-15 22:44:20\", \"2025-10-15 22:54:40\", \"udp\"),\n",
    "    (\"2025-10-16 00:10:40\", \"2025-10-16 00:19:40\", \"udp\")\n",
    "]\n",
    "ARQUIVO_BRUTO = \"datasets/MURILO/udp/test_features_1s.txt\" \n",
    "ARQUIVO_FINAL = \"datasets/MURILO/udp.csv\"\n",
    "\n",
    "\n",
    "# Chame a função passando o date_format\n",
    "filter_and_label_by_time(\n",
    "    raw_input_path=ARQUIVO_BRUTO,\n",
    "    final_output_path=ARQUIVO_FINAL,\n",
    "    global_start_str=PERIODO_INICIO,\n",
    "    global_end_str=PERIODO_FIM,\n",
    "    attacks_ground_truth=ATAQUES,\n",
    "    date_format=MEU_FORMATO,   \n",
    "    chunk_size=100000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
